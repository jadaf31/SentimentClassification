{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4edac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "with tarfile.open('aclImdb_v1.tar.gz', 'r:gz') as tar:\n",
    "    tar.extractall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1a07362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyprind in c:\\users\\jada\\anaconda3\\lib\\site-packages (2.11.3)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install pyprind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c31cbad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jada\\AppData\\Local\\Temp\\ipykernel_12716\\3244923863.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append([[txt, labels [l]]],\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:11:18\n"
     ]
    }
   ],
   "source": [
    "import pyprind\n",
    "import pandas as pd\n",
    "import os\n",
    "# change the 'basepath' to the directory of the\n",
    "# unzipped movie dataset\n",
    "basepath = 'aclImdb'\n",
    "\n",
    "labels = {'pos': 1, 'neg': 0}\n",
    "pbar = pyprind.ProgBar(50000)\n",
    "df = pd.DataFrame ()\n",
    "for s in ('test', 'train'):\n",
    "    for l in ('pos', 'neg'):\n",
    "        path = os.path.join(basepath, s, l)\n",
    "        for file in os.listdir(path):\n",
    "            with open (os.path.join (path, file),\n",
    "                        'r', encoding='utf-8') as infile:\n",
    "                txt = infile.read ()\n",
    "            df = df.append([[txt, labels [l]]],\n",
    "                            ignore_index=True)\n",
    "            pbar.update()\n",
    "df. columns = ['review','sentiment']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "057417ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random. seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "df.to_csv ('movie_data.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac6e2f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('movie_data.csv', encoding=\"utf-8\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed6eb1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer()\n",
    "docs = np.array([\n",
    "                'The sun is shining',\n",
    "                'The weather is sweet',\n",
    "                'The sun is shining and the weather is sweet'])\n",
    "bag = count.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6a75d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 5, 'sun': 3, 'is': 1, 'shining': 2, 'weather': 6, 'sweet': 4, 'and': 0}\n"
     ]
    }
   ],
   "source": [
    "print(count.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e1c412d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 1 0]\n",
      " [0 1 0 0 1 1 1]\n",
      " [1 2 1 1 1 2 1]]\n"
     ]
    }
   ],
   "source": [
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9200465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.43 0.56 0.56 0.   0.43 0.  ]\n",
      " [0.   0.43 0.   0.   0.56 0.43 0.56]\n",
      " [0.4  0.48 0.31 0.31 0.31 0.48 0.31]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer(use_idf=True,\n",
    "                        norm='l2',\n",
    "                        smooth_idf=True)\n",
    "np.set_printoptions(precision=2)\n",
    "print(tfidf.fit_transform(count.fit_transform(docs))\n",
    "        .toarray ())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0e8bbd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is seven.<br /><br />Title (Brazil): Not Available'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, 'review'][-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13273501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
    "                            text)\n",
    "    text =(re.sub ('[\\W]+', ' ', text.lower()) +\n",
    "            ' '.join(emoticons).replace ('-', ''))\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fc10b27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test :) :( :)'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor(\"</a>This :) is :( a test :-)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f0bd961",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1572fe6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runners', 'like', 'ruunning', 'and', 'thus', 'they', 'run']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "tokenizer('runners like ruunning and thus they run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7856de58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fb1e80e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jada\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48f8fa07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'and', 'thu', 'they', 'run']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "tokenizer_porter('runner like running and thus they run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38a6ceee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'run', 'lot']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "[w for w in tokenizer_porter('a runner likes running and runs a lot')[-10:] if w not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2887c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.loc [:25000, 'review'].values\n",
    "y_train = df.loc [:25000, 'sentiment'].values\n",
    "X_test = df.loc [25000:, 'review'].values\n",
    "y_test = df.loc [25000:, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab53f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                          lowercase=False, \n",
    "                         preprocessor=None)\n",
    "param_grid = [{'vect__ngram_range': [(1,1)],\n",
    "              'vect__stop_words': [stop, None],\n",
    "              'vect__tokenizer': [tokenizer,\n",
    "                                 tokenizer_porter],\n",
    "              'clf__penalty': ['l1', 'l2'],\n",
    "              'clf__C': [1.0, 10.0, 100.0]},\n",
    "                        {'vect__ngram_range': [(1,1)],\n",
    "                         'vect__stop_words': [stop, None],\n",
    "                         'vect__tokenizer': [tokenizer,\n",
    "                                            tokenizer_porter],\n",
    "                         'vect__use_idf': [False],\n",
    "                         'vect__norm': [None],\n",
    "                         'clf__penalty': ['l1', 'l2'],\n",
    "                         'clf__C': [1.0, 10.0, 100.0]}\n",
    "             ]\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                        ('clf',\n",
    "                         LogisticRegression(random_state=0))])\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=5, verbose=1,\n",
    "                           n_jobs=1)\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b5108c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best parameter set: is ' % gs_lr_tfidf.best_params_)\n",
    "Best parameter set: {'clf__C': 10.0, 'vect__stop_words': None,\n",
    "'clf__penalty': 'l2', 'vect_tokenizer': <function tokenizer at\n",
    "0x7f6c704948c8>, , 'vect__ngram _range': (1, 1)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dacf9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('CV Accuracy: S.3f'\n",
    "       % gs_lr_tfidf.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17f5933",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = gs_lr_tfidf.best_estimator_\n",
    "print ('Test Accuracy: %.3Â£'\n",
    "       % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0342e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
    "                            text.lower())\n",
    "    text =(re.sub ('[\\W]+', ' ', text.lower()) \\\n",
    "            + ' '.join(emoticons).replace ('-', ''))\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d58fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords('english')\n",
    "def stream_docs(path):\n",
    "    with open(path, 'r', encoding='utf-8') as csv:\n",
    "        next(csv) # skip header\n",
    "        next line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yeild text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32722f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(stram_docs(path = movie_data.csv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8f89e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "        except StopIteration:\n",
    "            return None, None\n",
    "        return docs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b218bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import Hashing Vectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "vect = HashingVector(decode_error='ignore',\n",
    "                    n_features=2**21,\n",
    "                    preprocessor=None,\n",
    "                    tokenizer=tokenizer)\n",
    "clf = SGDClassifier(loss='log', random_state=1, n_iter=1)\n",
    "doc_stream = stream_docs(path='movie_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f9d53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyprind\n",
    "pbar = pyprind.ProgBar(45)\n",
    "classes = np.array([0, 1])\n",
    "for _ in range(45):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size = 1000)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train - vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    pbar.update()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c9ed2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = getminibatch(doc_stream, size=5000)\n",
    "X_text = vect.transform(X_test)\n",
    "print('Accuracy: %.3f' % clf.score(X_test, y_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5824347",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = clf.prtial_fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f5cffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('movie_data.csv', encoding='utf-8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945650c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_etraction.text import CountVectorizer\n",
    "count = CountVectorizer(stop_words='english',\n",
    "                       max_df=.1,\n",
    "                       max_features=5000)\n",
    "X = count.fit_transform(df['review'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6509442b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_topics=10,\n",
    "                               random_state = 123,\n",
    "                               learning_method='batch')\n",
    "X_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed33e99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0746c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_words = 5\n",
    "feature_names = count.get_feature_names()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(\"Topic %d:\" % (topic_idx + 1))\n",
    "    print(\" \".join([feature_names[i]\n",
    "                   for i in topic.argsort()\\\n",
    "                       [:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f4257a",
   "metadata": {},
   "outputs": [],
   "source": [
    "horror = X_topics[:, 5].argsort()[::-1]\n",
    "for iter_idx, movie_idx in enumerate(horror[:3]):\n",
    "    print('\\nHorror movie #%d:' % (iter_idx + 1))\n",
    "    print (df['review'][movie_idx][:300], '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0a8aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "dest = os.path.join('movieclassifier', 'pk1 objects')\n",
    "if not os.path.exists(dest):\n",
    "    os.makedirs (dest)\n",
    "pickle.dump (stop,\n",
    "             open (os.path.join (dest, 'stopwords.pkl'), 'wb'),\n",
    "             protocol=4)\n",
    "pickle.dump (clf,\n",
    "             open (os.path.join (dest, 'classifier.pkl'), 'wb'),\n",
    "             protocol=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "453a65e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (616843549.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Jada\\AppData\\Local\\Temp\\ipykernel_23036\\616843549.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    from sklearn.feature_extraction.text\u001b[0m\n\u001b[1;37m                                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text \n",
    "import HashingVectorizer \n",
    "import re import os import pickle\n",
    "cur dir = os.path.dirname (\n",
    "file\n",
    ")\n",
    "stop = pickle.load (open (\n",
    "os.path.join(cur_dir,\n",
    "'pk1 objects'\n",
    "'stopwords.pkl'), 'rb'))\n",
    "def tokenizer (text):\n",
    "text = re.sub ('<[^>]*>',\n",
    "\"', text)\n",
    "emoticons = re.findall (' (?::I;|=) (?:-)?(?:111(IDIP)',\n",
    "text.lower ())\n",
    "text = re.sub('WIts,\n",
    "\"', text.lower ()) I\n",
    "+ \" \".join (emoticons). replace ('-', '\")\n",
    "tokenized = [w for w in text.split () if w not in stop]\n",
    "return tokenized\n",
    "vect = HashingVectorizer (decode error='ignore',\n",
    "n_features=2**21, preprocessor=None, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8dacb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b689793c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edcebb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2c3411",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52438f63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abada752",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ef2f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc4e487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e5c1a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a843b9a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301d64d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971b9825",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
